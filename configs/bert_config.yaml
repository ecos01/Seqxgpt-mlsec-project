# BERT Detector Configuration
# OPTIMIZED FOR FAST TRAINING ON CPU

# Model architecture
model:
  model_name: "distilbert-base-uncased"  # 2x faster than BERT, similar performance
  num_labels: 2
  dropout: 0.1

# Training parameters
training:
  batch_size: 32  # Larger batch for efficiency
  learning_rate: 0.00003  # 3e-5
  num_epochs: 3  # Minimal epochs, early stopping handles convergence
  max_length: 256  # Shorter = faster, sufficient for most texts
  early_stopping_patience: 1
  max_train_samples: 5000  # Reduced for very fast training (~15 min)
  max_val_samples: 1000
  gradient_accumulation_steps: 2

# Data configuration
data:
  data_dir: "dataset/SeqXGPT-Bench"
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  seed: 42
